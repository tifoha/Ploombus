###############################################/SEARCH ENGINE CONFIG##########################################################/
#path to index DB
engine.indexDirectoryPath = indexes

##############################################/SEARCHER CONFIG################################################################
##restriction to max result count
searcher.maxNumberOfResults = 1000

########################################################/INDEXER CONFIG####################################################
indexer.maxTimeBeforeTermination = 3000


##############################################/CRAWLER CONFIG################################################################/
# The folder which will be used by crawler for storing the intermediate
# crawl data. The content of this folder should not be modified manually.σσ
crawler.crawlStorageFolder = tmp/crawlers


# The number of concurrent threads which will be used by crawler for crawling the web
#crawler.numberOfCralwers = Runtime.getRuntime().availableProcessors()


# If this feature is enabled, you would be able to resume a previously
# cancelled/crashed crawl. However, it makes crawling slightly slower
crawler.resumableCrawling = false


# Maximum depth of crawling For unlimited depth this parameter should be
# set to -1
crawler.maxDepthOfCrawling = -1


# Maximum number of pages to fetch For unlimited number of pages, this
# parameter should be set to -1
crawler.maxPagesToFetch = -1


# user-agent string that is used for representing your crawler to web
# servers. See http://en.wikipedia.org/wiki/User_agent for more details
crawler.userAgentString = "Ploombus Crawler4j Bot"


# Politeness delay in milliseconds (delay between sending two requests to
# the same host).
crawler.politenessDelay = 200


# Should we also crawl https pages?
crawler.includeHttpsPages = true


# Should we fetch and process binary content such as images, audio, ...using TIKA?
crawler.includeBinaryContent = false


# Maximum Connections per host
crawler.maxConnectionsPerHost = 100


# Maximum total connections
crawler.maxTotalConnections = 100


# Socket timeout in milliseconds
crawler.socketTimeout = 20000


# Connection timeout in milliseconds
crawler.connectionTimeout = 30000


# Max number of outgoing links which are processed from a page
crawler.maxOutgoingLinksToFollow = 5000


# Max allowed size of a page. Pages/Binary content larger than this size will not be
# fetched.
crawler.maxDownloadSize = 1048576


# Should we follow redirects?
crawler.followRedirects = true


# Should the TLD list be updated automatically on each run? Alternatively,
# it can be loaded from the embedded tld-names.zip file that was obtained from
# https://publicsuffix.org/list/effective_tld_names.dat
crawler.onlineTldListUpdate = false


# If crawler should run behind a proxy, this parameter can be used for
# specifying the proxy host.
crawler.proxyHost = null


# If crawler should run behind a proxy, this parameter can be used for
# specifying the proxy port.
crawler.proxyPort = 80


# If crawler should run behind a proxy and user/pass is needed for
# authentication in proxy, this parameter can be used for specifying the
# username.
crawler.proxyUsername = null


# If crawler should run behind a proxy and user/pass is needed for
# authentication in proxy, this parameter can be used for specifying the
# password.
crawler.proxyPassword = null


######################################################################################################/
##    RobotstxtConfig


# Should the crawler obey Robots.txt protocol? More info on Robots.txt is
# available at http:/www.robotstxt.org/
enabledRobotstxt = true


# user-agent name that will be used to determine whether some servers have
# specific rules for this agent name.
crawler.robotstxt.userAgentNameRobotstxt = userAgentString


# The maximum number of hosts for which their robots.txt is cached.
crawler.robotstxt.cacheSizeRobotstxt = 500

# if crawl queue is empty, wait few seconds before shutdown crawler controller in milliseconds
crawler.robotstxt.delayBeforeShutdownCrawl = 1000



